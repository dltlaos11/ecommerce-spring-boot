# 부하 테스트 & 장애 대응 최종 발표 자료

## 📋 프로젝트 개요

### 목표

- **STEP 19**: 시스템 성능 한계 파악 및 부하 테스트 체계 구축
- **STEP 20**: 장애 대응 프로세스 수립 및 개선 방안 도출

### 완료 현황

✅ 부하 테스트 계획 수립  
✅ K6 테스트 스크립트 개발  
✅ 부하 테스트 실행  
✅ 성능 분석 보고서 작성  
✅ 장애 시나리오 문서화  
✅ 포스트모템 분석  
✅ 모니터링 전략 수립

---

## 🎯 STEP 19: 부하 테스트 결과

### 테스트 환경

```yaml
테스트 도구: K6 Load Testing Framework
목표 부하: 100 동시 사용자 (7분 테스트)
API 대상: 상품조회, 잔액조회, 주문생성, 쿠폰발급
시나리오: 실제 사용자 패턴 모델링
```

### 🚨 핵심 발견사항

#### 심각한 성능 문제 감지

- **임계점**: 2분 30초 후 대량 타임아웃 발생
- **시스템 한계**: ~50 동시 사용자 (예상의 절반)
- **실제 처리량**: 117 TPS (목표 200 TPS 대비 58%)

#### API별 성능 분석

| API      | 초기 응답시간 | 부하 시 상태 | 문제점           |
| -------- | ------------- | ------------ | ---------------- |
| 상품조회 | 93ms → 12ms   | ✅ 안정적    | 캐시 효과로 우수 |
| 잔액조회 | 45ms          | ✅ 안정적    | 단순 조회로 양호 |
| 주문생성 | 66ms          | ❌ 400 에러  | 데이터 검증 실패 |
| 쿠폰발급 | -             | ❌ 타임아웃  | 가장 심각한 병목 |

### 🔍 근본 원인 분석

#### 1순위: DB 커넥션 풀 고갈

```yaml
현재 설정: HikariCP 16개 커넥션
부하 상황: 100% 포화 → 신규 요청 대기
해결 방안: 50개 이상 확대 필요
```

#### 2순위: 쿠폰 시스템 복잡도

```yaml
처리 과정: Redis → DB → Kafka (3단계)
병목 지점: Consumer 처리 속도 < Producer 생성 속도
개선 방안: 비동기 처리 최적화 필요
```

---

## 🚨 STEP 20: 장애 대응 시나리오

### 장애 시뮬레이션

```yaml
장애명: 쿠폰 이벤트 중 시스템 과부하
발생일시: 2025-09-12 14:00-14:15 (15분간)
심각도: Critical (전체 서비스 영향)
영향범위: 2,500명 동시 접속자
```

### 📊 비즈니스 임팩트

```yaml
실패 쿠폰 발급: 1,200건 (95% 실패율)
실패 주문: 150건
매출 손실: 약 180만원
고객 문의: 350건 급증 (350% 증가)
```

### ⏰ 장애 타임라인 (핵심 구간)

#### 14:00:30 - 트래픽 급증

- 동시접속자: 150명 → 800명 (30초 만에)
- TPS: 30 → 200

#### 14:01:30 - 임계점 돌파

- 동시접속자: 1,500명
- DB 커넥션: 16/16개 완전 포화
- 응답시간: 8초 이상

#### 14:02:00 - 전면 장애 시작

- 모든 API: 타임아웃 발생
- 쿠폰 발급: 99% 실패율
- 에러 로그 폭증

#### 14:03:00 - 긴급 조치 시작

1. DB 커넥션 풀: 16 → 50개 확대
2. 쿠폰 이벤트 일시 중단
3. Rate limiting 적용

#### 14:15:00 - 완전 복구

- 모든 서비스 정상 동작
- 성능 지표 정상 범위 복귀

### 📈 장애 대응 성과지표

```yaml
MTTD (장애 감지 시간): 2분 30초
MTTR (복구 시간): 12분 30초
서비스 가용성: 83% (15분 중 12분 30초 영향)
최종 쿠폰 발급 성공률: 96% (재개 후)
```

---

## 💡 개선 방안 & 교훈

### 🔥 즉시 적용 (Critical)

1. **DB 커넥션 풀 확대**: 16 → 50개
2. **쿠폰 시스템 최적화**: Kafka Consumer 3 → 10
3. **모니터링 강화**: 실시간 알림 체계 구축

### 📈 단기 개선 (1주 내)

1. **인덱스 최적화**: 슬로우 쿼리 해결
2. **캐시 계층 강화**: Redis 활용 확대
3. **부하 테스트 정례화**: 주 1회 실행

### 🏗️ 장기 개선 (1개월 내)

1. **서비스 분리**: 쿠폰 시스템 독립화
2. **자동 스케일링**: 클라우드 네이티브 전환
3. **카오스 엔지니어링**: 장애 내성 강화

### 🎯 핵심 교훈

> **"준비되지 않은 성공이 가장 위험한 실패다"**

- 마케팅 성공 → 기술적 장애의 역설적 상황
- 성능 테스트의 중요성 재확인
- 예측 불가능한 트래픽 급증 대비 필요
- 모니터링과 자동화의 중요성 입증

---

## 📊 모니터링 전략

### 3계층 모니터링 아키텍처

#### Layer 1: 인프라 모니터링

```yaml
대상: CPU, 메모리, 디스크, 네트워크
도구: Prometheus + Grafana
알림: 임계값 초과 시 즉시 알림
```

#### Layer 2: 애플리케이션 모니터링

```yaml
대상: 응답시간, 에러율, TPS, DB커넥션
도구: Spring Boot Actuator + Micrometer
알림: SLA 위반 시 자동 알림
```

#### Layer 3: 비즈니스 모니터링

```yaml
대상: 주문성공률, 쿠폰발급률, 매출영향
도구: 커스텀 메트릭 + 대시보드
알림: 비즈니스 KPI 이상 시 알림
```

### SLA/SLI 정의

```yaml
가용성 SLA: 99.9% (월 43분 다운타임 허용)
응답시간 SLA: P95 < 2초
에러율 SLA: < 0.1%
처리량 SLA: > 200 TPS
```

---

## 🚀 다음 단계

### 즉시 실행 항목

- [ ] DB 커넥션 풀 설정 변경 적용
- [ ] 모니터링 알림 임계값 재조정
- [ ] 부하 테스트 주간 정례화
- [ ] 장애 대응 플레이북 팀 공유

### 검증 계획

```yaml
재테스트 일정: 개선 적용 후 1주 내
목표: 100 동시 사용자 안정적 처리
검증 방법: 동일 시나리오 부하 테스트
성공 기준: 에러율 < 1%, 응답시간 P95 < 2초
```

### 장기 로드맵

1. **Q1**: 인프라 자동 스케일링 구축
2. **Q2**: 마이크로서비스 아키텍처 전환
3. **Q3**: 카오스 엔지니어링 도입
4. **Q4**: SRE 조직 구성 완료

---

## 📞 관련 문서

- 📄 [부하 테스트 계획서](../load-test/test-plan.md)
- 📊 [성능 분석 보고서](../load-test/performance-analysis-report.md)
- 🚨 [장애 시나리오](../incident-response/incident-scenario.md)
- 📝 [포스트모템 문서](../incident-response/postmortem.md)
- 📈 [모니터링 전략](../monitoring/monitoring-strategy.md)

**🎯 결론**: 시스템의 한계를 명확히 파악하고, 체계적인 개선 방안을 수립했습니다. 이제 안정적이고 확장 가능한 서비스 운영을 위한 기반이 마련되었습니다.
