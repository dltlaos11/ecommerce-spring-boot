# 장애 대응 시나리오: 쿠폰 이벤트 중 시스템 과부하 장애

## 📋 장애 개요

### 기본 정보

- **장애 ID**: INC-2025-0912-001
- **발생 일시**: 2025-09-12 14:00 - 14:15 (15분간)
- **심각도**: Critical (전체 서비스 영향)
- **장애 유형**: 시스템 과부하로 인한 전면 서비스 장애
- **영향 범위**: 전체 API 서비스 (상품, 주문, 쿠폰, 잔액)

### 비즈니스 임팩트

- **영향받은 사용자**: 약 2,500명 (동시 접속자)
- **실패한 쿠폰 발급**: 1,200건 (전체 요청의 95%)
- **실패한 주문**: 150건
- **매출 손실**: 약 180만원 (평균 주문액 12,000원 × 150건)
- **고객 문의**: 350건 급증

## 🎯 장애 시나리오 배경

### 이벤트 설정

- **이벤트명**: "신학기 특가 쿠폰 이벤트"
- **쿠폰 수량**: 500장 (선착순)
- **할인율**: 30% (최대 5,000원)
- **시작 시간**: 2025-09-12 14:00 (점심시간 직후)
- **예상 참여자**: 800명 → **실제 참여자**: 2,500명 (3배 초과)

### 시스템 상태 (장애 발생 전)

```yaml
시스템 리소스:
  - CPU 사용률: 25% (평상시)
  - 메모리 사용률: 45%
  - DB 커넥션: 3/16개 사용 중
  - Redis 연결: 정상
  - Kafka 상태: 정상

트래픽 패턴:
  - 평상시 TPS: 20-30
  - 동시 접속자: 150명
  - 응답시간: P95 500ms 이하
```

## ⏰ 상세 장애 타임라인

### 14:00:00 - 🎉 이벤트 시작

- 쿠폰 이벤트 공지 및 링크 오픈
- 소셜미디어 확산으로 예상보다 높은 관심

### 14:00:30 - 📈 트래픽 급증 시작

```yaml
지표 변화:
  - 동시 접속자: 150명 → 800명 (30초 만에)
  - TPS: 30 → 200
  - 쿠폰 API 요청: 초당 150건
```

### 14:01:00 - ⚠️ 첫 번째 경고 신호

```yaml
시스템 상태:
  - CPU 사용률: 65% 급등
  - DB 커넥션: 12/16개 사용
  - 쿠폰 API 응답시간: 500ms → 1.5초
  - Redis 큐 크기: 50 → 200개
```

**로그 메시지**:

```
WARN: HikariPool-1 - Connection is not available, request timed out after 30000ms
INFO: Redis connection pool near capacity: 14/16 connections active
```

### 14:01:30 - 🔥 임계점 돌파

```yaml
심각한 지표:
  - 동시 접속자: 1,500명
  - TPS: 400 (한계 초과)
  - DB 커넥션: 16/16개 완전 포화
  - 쿠폰 API 응답시간: 8초+
```

**자동 알림 발생**:

```
ALERT: Database connection pool exhausted
ALERT: Response time threshold exceeded (>5s)
ALERT: Error rate above 10%
```

### 14:02:00 - 🚨 시스템 장애 시작

```yaml
장애 증상:
  - 모든 API: 타임아웃 발생
  - 쿠폰 발급: 99% 실패율
  - 주문 생성: 95% 실패율
  - 상품 조회: 50% 실패율
```

**에러 로그 폭증**:

```
ERROR: Request timeout in client
ERROR: Connection acquisition timeout
ERROR: Redis command timed out
ERROR: Kafka producer send failed
```

### 14:02:30 - 📞 장애 감지 및 대응 시작

**개발팀 긴급 소집**:

- Slack 알림으로 장애 상황 전파
- 모니터링 대시보드 확인
- 로그 분석 시작

**초기 분석 결과**:

```yaml
근본 원인 후보:
1. DB 커넥션 풀 고갈
2. Redis 연결 한계 도달
3. 쿠폰 시스템 처리 속도 < 요청 속도
4. JVM 메모리 부족 (GC 지연)
```

### 14:03:00 - 🚑 1차 긴급 조치

**즉시 실행된 조치**:

1. **DB 커넥션 풀 긴급 확대**

   ```bash
   # application.yml 수정 후 재시작 없이 적용
   HikariCP 커넥션: 16 → 50개
   ```

2. **쿠폰 이벤트 일시 중단**

   ```bash
   # Redis에서 쿠폰 재고를 0으로 설정
   REDIS: SET coupon:1:stock 0
   ```

3. **로드밸런서 설정 변경**
   ```bash
   # 건전하지 않은 요청 차단
   Rate limiting: 100 req/min per IP
   ```

### 14:05:00 - 📊 부분적 복구 시작

```yaml
개선 지표:
  - DB 커넥션 사용률: 100% → 60%
  - 상품 조회 API: 복구 (응답시간 2초)
  - 잔액 조회 API: 복구
  - 쿠폰 API: 여전히 불안정 (5초+ 응답시간)
```

### 14:07:00 - 🔧 2차 세부 조치

**Kafka Consumer 최적화**:

```yaml
기존: concurrency = 3
변경: concurrency = 10
효과: 메시지 처리 속도 3배 향상
```

**Redis 연결 튜닝**:

```yaml
기존: max-active = 16
변경: max-active = 50
효과: Redis 커넥션 대기 시간 감소
```

**JVM 가비지 컬렉션 튜닝**:

```bash
# 기존 G1GC 설정 최적화
-XX:G1HeapRegionSize=16m
-XX:MaxGCPauseMillis=100
```

### 14:10:00 - ✅ 서비스 안정화

```yaml
정상 복구 지표:
  - 전체 API 응답시간: P95 < 2초
  - 에러율: < 1%
  - DB 커넥션: 안정적 (30-40% 사용률)
  - Redis: 정상
  - 동시 접속자: 800명 (안정적 처리)
```

### 14:12:00 - 🎫 쿠폰 이벤트 재개

**안전한 재개 전략**:

```yaml
점진적 재오픈:
  - 1분차: 100장 한정 오픈
  - 3분차: 추가 200장 오픈
  - 5분차: 나머지 200장 오픈

결과:
  - 성공적인 쿠폰 발급: 480장 (96%)
  - 평균 응답시간: 1.2초
  - 사용자 만족도: 복구 확인
```

### 14:15:00 - 📝 장애 종료 선언

**최종 상태 확인**:

```yaml
서비스 정상성:
  - 모든 API 정상 동작 확인
  - 성능 지표 정상 범위 복귀
  - 고객 문의 감소 확인
  - 추가 장애 징후 없음
```

## 📊 장애 영향 분석

### 정량적 임팩트

```yaml
성능 지표:
  - MTTD (Mean Time To Detect): 2분 30초
  - MTTR (Mean Time To Recover): 12분 30초
  - 최대 에러율: 99% (쿠폰 API)
  - 서비스 가용성: 83% (15분 중 12분 30초 영향)

비즈니스 영향:
  - 실패한 쿠폰 발급: 1,200건 → 사용자 불만
  - 실패한 주문: 150건 → 직접적 매출 손실
  - 브랜드 신뢰도 하락: 소셜미디어 부정 반응
  - 고객센터 부하: 350% 증가
```

### 근본 원인 분석 (5 Whys)

**Q1. 왜 시스템이 장애가 발생했나?**

- A1. DB 커넥션 풀이 모두 소진되어 새로운 요청을 처리할 수 없었다.

**Q2. 왜 DB 커넥션 풀이 소진되었나?**

- A2. 예상보다 3배 많은 트래픽 (2,500명)이 몰리면서 커넥션 수요가 급증했다.

**Q3. 왜 트래픽 예측이 부정확했나?**

- A3. 소셜미디어 확산 효과를 고려하지 않았고, 과거 이벤트 데이터가 부족했다.

**Q4. 왜 시스템이 트래픽 급증에 대비하지 못했나?**

- A4. 부하 테스트를 충분히 수행하지 않았고, 자동 스케일링 메커니즘이 없었다.

**Q5. 왜 부하 테스트가 부족했나?**

- A5. 개발 일정에 쫓겨 성능 테스트를 간소화했고, 실제 트래픽 패턴을 반영하지 못했다.

## 🛠️ 즉시 적용된 해결 조치

### 1. 인프라 개선

```yaml
DB 커넥션 풀:
  기존: HikariCP 16개
  변경: 50개
  모니터링: 사용률 80% 초과 시 알림

Redis 연결:
  기존: max-active 16개
  변경: 50개
  타임아웃: 2초 → 5초

Kafka Consumer:
  기존: concurrency 3
  변경: 10
  배치 크기: 최적화
```

### 2. 애플리케이션 최적화

```java
// 쿠폰 발급 로직 개선
@Async
public CompletableFuture<CouponResponse> issueCouponAsync(Long userId, Long couponId) {
    // 빠른 응답 우선, 백그라운드에서 실제 처리
    return CompletableFuture.completedFuture(
        CouponResponse.accepted(generateRequestId())
    );
}

// DB 쿼리 최적화
@Query("SELECT p FROM Product p WHERE p.stock > 0")
List<Product> findAvailableProducts();
```

### 3. 모니터링 강화

```yaml
알림 설정:
  - DB 커넥션 사용률 > 70%
  - 응답시간 P95 > 2초
  - 에러율 > 5%
  - Redis 메모리 사용률 > 80%
  - Kafka Consumer Lag > 100

대시보드:
  - 실시간 TPS 모니터링
  - API별 응답시간 분포
  - 에러율 트렌드
  - 시스템 리소스 현황
```

## 📚 교훈 및 개선 계획

### ✅ 잘한 점

1. **빠른 감지**: 2분 30초 내 장애 상황 파악
2. **체계적 대응**: 우선순위에 따른 단계별 조치
3. **팀워크**: 개발팀 신속한 협업 및 역할 분담
4. **투명한 소통**: 사용자에게 실시간 상황 공유

### 🔄 개선할 점

1. **예방적 모니터링**: 부하 증가 초기 단계에서 미리 감지
2. **용량 계획**: 트래픽 예측 및 리소스 사전 확보
3. **자동화**: 수동 조치를 자동화된 스케일링으로 대체
4. **테스트 강화**: 정기적인 부하 테스트 및 장애 시뮬레이션

### 📋 후속 조치 계획

#### 단기 계획 (1주 내)

- [ ] 부하 테스트 정기 실행 (주 1회)
- [ ] 모니터링 알림 임계값 재조정
- [ ] 장애 대응 플레이북 업데이트
- [ ] 쿠폰 시스템 아키텍처 재검토

#### 중기 계획 (1개월 내)

- [ ] 자동 스케일링 시스템 구축
- [ ] DB 성능 최적화 (인덱스, 쿼리 튜닝)
- [ ] 캐시 전략 전면 재설계
- [ ] 서비스 분리 아키텍처 검토

#### 장기 계획 (3개월 내)

- [ ] 마이크로서비스 아키텍처 전환
- [ ] 클라우드 네이티브 환경 구축
- [ ] 카오스 엔지니어링 도입
- [ ] SRE 조직 구성 검토

---

**📞 관련 문서**:

- [포스트모템 문서](postmortem.md)
- [모니터링 전략](../monitoring/monitoring-strategy.md)
