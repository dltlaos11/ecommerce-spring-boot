# 포스트모템: 쿠폰 이벤트 시스템 과부하 장애

## 📊 장애 요약

- **장애 ID**: INC-2025-0912-001
- **발생 일시**: 2025-09-12 14:00 - 14:15 (15분간)
- **심각도**: Critical
- **영향 범위**: 전체 API 서비스
- **근본 원인**: 예상 트래픽 초과로 인한 DB 커넥션 풀 고갈
- **MTTD**: 2분 30초 | **MTTR**: 12분 30초

## 🎯 배경 및 맥락

### 사건 개요

신학기 특가 쿠폰 이벤트 진행 중, 예상보다 3배 많은 사용자(2,500명)가 몰리면서 시스템 전체가 과부하 상태에 빠진 장애 상황. 소셜미디어를 통한 급속한 확산이 주요 원인으로 분석됨.

### 시스템 환경

```yaml
인프라:
  - 단일 서버 환경 (Spring Boot)
  - MySQL 8.0 (로컬)
  - Redis 클러스터 (6노드)
  - Kafka (단일 브로커)

리소스 제한:
  - DB 커넥션 풀: 16개 (HikariCP)
  - Redis 연결: 16개
  - JVM 힙: 1GB
  - CPU: 제한 없음
```

### 트리거 이벤트

- 점심시간 직후(14:00) 쿠폰 이벤트 시작
- SNS를 통한 바이럴 확산 (예상 외)
- 30초 만에 동시 접속자 150명 → 800명으로 급증

## ⏰ 상세 타임라인

### 14:00:00 - 이벤트 시작

```
🎉 쿠폰 이벤트 시작 공지
📱 소셜미디어 공유 시작
📊 초기 지표: 정상 (TPS: 30, 접속자: 150명)
```

### 14:00:30 - 트래픽 급증 감지

```
📈 동시 접속자: 150 → 800명 (30초 만에)
⚡ TPS: 30 → 200 (6배 증가)
🎫 쿠폰 API 요청: 초당 150건
```

### 14:01:30 - 임계점 돌파

```yaml
🚨 핵심 지표 악화:
  - CPU 사용률: 25% → 65%
  - DB 커넥션: 3/16 → 16/16 (완전 포화)
  - 응답시간: 500ms → 8초+
  - 에러율: 0% → 30%
```

**자동 알림 발생**:

```
ALERT: Database connection pool exhausted
ALERT: Response time SLA violated (>2s)
ALERT: High error rate detected (>10%)
```

### 14:02:00 - 전면 장애 시작

```yaml
💥 시스템 마비:
  - 쿠폰 발급: 99% 실패
  - 주문 생성: 95% 실패
  - 상품 조회: 50% 실패
  - 전체 서비스: 사실상 중단
```

### 14:02:30 - 장애 대응 시작

```
👥 개발팀 긴급 소집 (Slack 알림)
🔍 모니터링 대시보드 확인
📊 로그 분석 및 원인 파악
```

**초기 진단 결과**:

1. DB 커넥션 풀 완전 고갈 확인
2. Redis 연결 한계 근접
3. JVM GC 빈발 발생
4. Kafka Consumer Lag 급증

### 14:03:00 - 1차 긴급 조치

```yaml
🚑 즉시 실행:
1. DB 커넥션 풀: 16 → 50개 확대
2. 쿠폰 이벤트 일시 중단
3. Rate limiting 적용 (100 req/min)
```

### 14:05:00 - 부분 복구 시작

```yaml
📊 개선 지표:
  - DB 커넥션 사용률: 100% → 60%
  - 상품/잔액 API: 복구 완료
  - 쿠폰 API: 여전히 불안정
```

### 14:07:00 - 2차 최적화 조치

```yaml
🔧 세부 튜닝:
  - Kafka Consumer 동시성: 3 → 10
  - Redis 연결 풀: 16 → 50개
  - JVM GC 파라미터 최적화
```

### 14:10:00 - 서비스 안정화

```yaml
✅ 정상 복구:
  - 전체 API P95 < 2초
  - 에러율 < 1%
  - 안정적 800명 동시 처리
```

### 14:12:00 - 이벤트 안전 재개

```yaml
📈 점진적 재오픈:
  - 1분: 100장 → 성공률 98%
  - 3분: 추가 200장 → 성공률 97%
  - 5분: 나머지 200장 → 성공률 95%
```

### 14:15:00 - 장애 종료 선언

```
🎯 완전 복구 확인
📞 고객센터 문의 정상화
📊 모든 지표 안정 범위 복귀
```

## 🔍 근본 원인 분석

### 직접적 원인

```yaml
기술적 원인:
  - DB 커넥션 풀 크기 부족 (16개 → 2,500 사용자)
  - Redis 연결 풀 한계 도달
  - Kafka Consumer 처리 속도 < Producer 생성 속도
  - JVM 메모리 압박으로 인한 GC 지연
```

### 근본적 원인 (5 Whys 분석)

**Q1. 왜 DB 커넥션 풀이 고갈되었나?**

- A1: 예상보다 3배 많은 동시 사용자가 몰렸기 때문

**Q2. 왜 트래픽 예측이 빗나갔나?**

- A2: 소셜미디어 확산 효과를 과소평가했고, 충분한 부하 테스트를 하지 않았기 때문

**Q3. 왜 부하 테스트가 부족했나?**

- A3: 개발 일정에 쫓겨 성능 테스트를 간소화하고, 실제 사용자 패턴을 반영하지 못했기 때문

**Q4. 왜 개발 일정에 쫓겼나?**

- A4: 비즈니스 요구사항 변경이 빈번했고, 기술 부채 해결을 미뤘기 때문

**Q5. 왜 기술 부채 해결을 미뤘나?**

- A5: 단기 성과에만 집중하고, 장기적인 안정성 투자를 소홀히 했기 때문

### 기여 요인들

```yaml
환경적 요인:
  - 점심시간 직후 최적 타이밍 (사용자 활성도 높음)
  - 할인율 30% (높은 매력도)
  - 선착순 500장 (희소성 심리)

조직적 요인:
  - 성능 테스트 프로세스 부재
  - 모니터링 알림 임계값 부적절
  - 장애 대응 시뮬레이션 경험 부족

기술적 요인:
  - 단일 서버 아키텍처 (SPOF)
  - 수동 스케일링 의존
  - 캐시 전략 미흡
```

## 📊 장애 영향 분석

### 정량적 임팩트

```yaml
기술 지표:
  - 서비스 가용성: 83% (목표 99.9%)
  - 최대 응답시간: 30초+ (목표 2초)
  - 에러율: 99% (목표 0.1%)
  - 복구 시간: 15분 (목표 5분)

비즈니스 지표:
  - 실패한 쿠폰 발급: 1,200건
  - 실패한 주문: 150건
  - 매출 손실: 180만원
  - 고객 문의: 350건 (평소 대비 350%)
```

### 정성적 영향

```yaml
긍정적 영향:
  - 팀 협업 능력 향상
  - 장애 대응 역량 강화
  - 시스템 한계 정확한 파악
  - 성능 최적화 노하우 축적

부정적 영향:
  - 브랜드 신뢰도 일시적 하락
  - 고객센터 업무 과부하
  - 개발팀 스트레스 증가
  - 향후 이벤트 기획 시 제약
```

## 🛠️ 해결 조치

### ✅ 즉시 적용된 조치 (완료)

1. **인프라 리소스 확대**

   ```yaml
   DB 커넥션 풀:
     - 기존: HikariCP 16개
     - 변경: 50개 (3배 증가)
     - 모니터링: 사용률 70% 초과 시 알림

   Redis 연결:
     - 기존: 16개
     - 변경: 50개
     - 타임아웃: 2초 → 5초

   Kafka Consumer:
     - 기존: concurrency 3
     - 변경: 10 (처리 속도 3배 향상)
   ```

2. **모니터링 및 알림 강화**

   ```yaml
   새 알림 설정:
     - DB 커넥션 > 70% (기존: 90%)
     - 응답시간 P95 > 2초 (기존: 5초)
     - 에러율 > 5% (기존: 10%)
     - TPS > 200 (신규)

   대시보드 개선:
     - 실시간 커넥션 사용률 시각화
     - API별 응답시간 분포 차트
     - 에러 로그 실시간 스트림
   ```

3. **애플리케이션 최적화**

   ```java
   // 쿠폰 발급 로직 개선
   @Transactional(timeout = 30)
   public CouponIssueResponse issueCoupon(Long userId, Long couponId) {
       // 빠른 검증 후 즉시 응답
       validateCouponEligibility(userId, couponId);

       // 비동기로 실제 처리
       couponProcessor.processAsync(userId, couponId);

       return CouponIssueResponse.accepted();
   }

   // 연결 풀 설정 최적화
   spring:
     datasource:
       hikari:
         maximum-pool-size: 50
         minimum-idle: 10
         connection-timeout: 20000
         leak-detection-threshold: 30000
   ```

### 📋 단기 개선 계획 (1주 내)

- [ ] **부하 테스트 자동화**

  - 매주 금요일 자동 실행
  - 100/200/500 사용자 시나리오
  - 결과를 Slack으로 자동 리포트

- [ ] **장애 대응 플레이북 작성**

  - 단계별 대응 절차 문서화
  - 권한 및 연락처 정리
  - 주요 명령어 스크립트화

- [ ] **DB 성능 최적화**

  ```sql
  -- 자주 사용되는 쿼리 인덱스 추가
  CREATE INDEX idx_user_coupons_issued_at ON user_coupons(user_id, created_at);
  CREATE INDEX idx_orders_user_status ON orders(user_id, status);

  -- 슬로우 쿼리 로그 활성화
  SET GLOBAL slow_query_log = 'ON';
  SET GLOBAL long_query_time = 1;
  ```

- [ ] **캐시 전략 개선**

  ```java
  @Cacheable(value = "products", key = "#available")
  public List<Product> getProducts(boolean available) {
      // 상품 목록 캐시 (TTL: 5분)
  }

  @CacheEvict(value = "coupon", key = "#couponId")
  public void updateCouponStock(Long couponId) {
      // 쿠폰 재고 변경 시 캐시 무효화
  }
  ```

### 🏗️ 중장기 개선 계획 (1-3개월)

- [ ] **자동 스케일링 시스템 구축**

  ```yaml
  Auto Scaling 정책:
    - CPU 70% 초과 시 인스턴스 추가
    - DB 커넥션 80% 초과 시 풀 크기 확대
    - Queue 길이 100 초과 시 Consumer 추가
  ```

- [ ] **서비스 분리 아키텍처**

  ```yaml
  마이크로서비스 분리:
    - 쿠폰 서비스 독립화
    - 주문 서비스 분리
    - 각 서비스별 DB 스키마
    - API Gateway 도입
  ```

- [ ] **클라우드 환경 전환**

  ```yaml
  클라우드 인프라:
    - 컨테이너화 (Docker + Kubernetes)
    - 관리형 데이터베이스 (RDS)
    - 로드밸런서 및 CDN
    - 서버리스 함수 활용
  ```

- [ ] **관찰 가능성(Observability) 강화**
  ```yaml
  모니터링 스택:
    - 메트릭: Prometheus + Grafana
    - 로깅: ELK Stack
    - 트레이싱: Jaeger
    - 알림: PagerDuty
  ```

## 📚 교훈 및 인사이트

### ✅ 잘한 점

1. **빠른 감지와 대응**

   - 2분 30초 내 장애 상황 파악
   - 체계적인 단계별 복구 조치
   - 팀원 간 효율적인 역할 분담

2. **투명한 커뮤니케이션**

   - 사용자에게 실시간 상황 공유
   - 내부 팀 간 정보 신속 전파
   - 고객센터와의 원활한 협조

3. **데이터 기반 의사결정**
   - 모니터링 지표 기반 원인 분석
   - 정량적 임팩트 측정
   - 근거 있는 해결 방안 도출

### 🔄 개선이 필요한 점

1. **예방적 접근 부족**

   - 사후 대응 위주의 사고방식
   - 부하 테스트 프로세스 부재
   - 트래픽 예측 능력 부족

2. **아키텍처 한계**

   - 단일 장애점(SPOF) 다수 존재
   - 수동 스케일링 의존도 높음
   - 서비스 간 결합도 과도

3. **조직적 프로세스**
   - 성능 요구사항 정의 미흡
   - 장애 대응 훈련 부족
   - 기술 부채 관리 체계 없음

### 💡 핵심 인사이트

#### 1. "성공의 역설"

```
마케팅의 성공이 기술적 실패로 이어지는 아이러니.
비즈니스 성장과 기술 역량의 균형이 얼마나 중요한지 깨달음.
예상보다 큰 성공에 대비하는 것도 리스크 관리의 일부.
```

#### 2. "작은 병목, 큰 영향"

```
16개 DB 커넥션이라는 작은 제약이 전체 시스템을 마비시킴.
시스템에서 가장 약한 고리가 전체 성능을 결정한다는 법칙 재확인.
인프라의 모든 구성 요소를 균형 있게 설계해야 함.
```

#### 3. "모니터링의 진정한 가치"

```
장애 발생 후 문제를 찾는 것이 아니라,
장애 발생 전 징후를 감지하는 것이 진짜 모니터링.
알림 임계값 설정이 얼마나 중요한지 경험으로 학습.
```

#### 4. "팀워크의 힘"

```
개인의 기술력보다 팀의 협업 능력이 장애 대응의 핵심.
평소 커뮤니케이션 문화가 위기 상황에서 빛을 발함.
투명하고 비난 없는 문화가 빠른 문제 해결을 가능하게 함.
```

## 📋 액션 아이템

### 즉시 실행 (완료)

- [x] DB 커넥션 풀 확대 (16→50개)
- [x] Redis 연결 풀 확대 (16→50개)
- [x] Kafka Consumer 동시성 증가 (3→10)
- [x] 모니터링 알림 임계값 조정

### 1주 내 (진행 중)

- [ ] 부하 테스트 자동화 스크립트 작성 (담당: 개발팀, 마감: 09-19)
- [ ] 장애 대응 플레이북 문서화 (담당: DevOps, 마감: 09-18)
- [ ] DB 인덱스 최적화 (담당: DBA, 마감: 09-20)
- [ ] 슬로우 쿼리 모니터링 설정 (담당: 인프라팀, 마감: 09-17)

### 1개월 내 (계획)

- [ ] 자동 스케일링 시스템 구축 (담당: 인프라팀, 마감: 10-15)
- [ ] 캐시 전략 전면 재설계 (담당: 개발팀, 마감: 10-10)
- [ ] API 응답 시간 SLA 정의 (담당: 전체팀, 마감: 10-05)
- [ ] 장애 대응 시뮬레이션 훈련 (담당: 전체팀, 마감: 10-30)

### 3개월 내 (로드맵)

- [ ] 마이크로서비스 아키텍처 설계 (담당: 아키텍트팀)
- [ ] 클라우드 마이그레이션 계획 수립 (담당: 인프라팀)
- [ ] 관찰 가능성 스택 구축 (담당: DevOps팀)
- [ ] SRE 조직 구성 검토 (담당: 경영진)

## 📞 관련 문서 및 자료

- [부하 테스트 보고서](../load-test/performance-analysis-report.md)
- [장애 시나리오 상세](incident-scenario.md)
- [모니터링 전략](../monitoring/monitoring-strategy.md)
